{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data from current time, we use the praw to get submissions. But since the number of submissions are limited, and the Reddit API removed its timestamp feature, to get previous year data we make use of the Reddit Data made available by Jason Michael Baumgartner using Google BigQuery.\n",
    "So, two approaches are used to collect data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I:\n",
    "\n",
    "Import the libraries needed for Reddit Data Collection :\n",
    "1. praw - (“Python Reddit API Wrapper”, a python package that allows for simple access to reddit's API.)\n",
    "2. pandas - open source data analysis and manipulation tool\n",
    "3. matplotlib and seaborn to visualise data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Reddit instance to access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit=praw.Reddit(client_id='',\n",
    "                   client_secret='',\n",
    "                   username='',\n",
    "                   password='',\n",
    "                   user_agent=''\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the subreddit - r/india and create a list to hold the all the flair needed. ( The allowed flairs are mentioned - https://www.reddit.com/r/india/wiki/rules )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit=reddit.subreddit('india')\n",
    "\n",
    "flair_list=['AskIndia','Non-Political','Scheduled','Photography','Science/Technology','Politics','Business/Finance','Policy/Economy','Sports','Food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the attributes available with the submissions available in a subreddit to see which ones can be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STR_FIELD', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_chunk', '_comments_by_id', '_fetch', '_fetch_data', '_fetch_info', '_fetched', '_kind', '_reddit', '_reset_attributes', '_safely_add_arguments', '_url_parts', '_vote', 'all_awardings', 'allow_live_comments', 'approved_at_utc', 'approved_by', 'archived', 'author', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders', 'banned_at_utc', 'banned_by', 'can_gild', 'can_mod_post', 'category', 'clear_vote', 'clicked', 'comment_limit', 'comment_sort', 'comments', 'content_categories', 'contest_mode', 'created', 'created_utc', 'crosspost', 'delete', 'disable_inbox_replies', 'discussion_type', 'distinguished', 'domain', 'downs', 'downvote', 'duplicates', 'edit', 'edited', 'enable_inbox_replies', 'flair', 'fullname', 'gild', 'gilded', 'gildings', 'hidden', 'hide', 'hide_score', 'id', 'id_from_url', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'likes', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'mark_visited', 'media', 'media_embed', 'media_only', 'mod', 'mod_note', 'mod_reason_by', 'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'num_reports', 'over_18', 'parent_whitelist_status', 'parse', 'permalink', 'pinned', 'post_hint', 'preview', 'pwls', 'quarantine', 'removal_reason', 'removed_by', 'removed_by_category', 'reply', 'report', 'report_reasons', 'save', 'saved', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'selftext_html', 'send_replies', 'shortlink', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'total_awards_received', 'treatment_tags', 'unhide', 'unsave', 'ups', 'upvote', 'url', 'user_reports', 'view_count', 'visited', 'whitelist_status', 'wls']\n"
     ]
    }
   ],
   "source": [
    "submissions=subreddit.search(flair_list[0],limit=1)\n",
    "for submission in submissions:\n",
    "    print(dir(submission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas adataframe to hold the attributes to save and use for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['flair','title','author','text','url','comments','score','domain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a for loop to get the information for the various Flair and store them into the database (max. 200 for each flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flair in flair_list:\n",
    "    list_of_submission=subreddit.search(flair,limit=200)\n",
    "    for submission in list_of_submission:\n",
    "        if not submission.stickied:\n",
    "            comments=\"\"\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            comment_list=submission.comments.list()\n",
    "            for comment in comment_list:\n",
    "                comments=comments+'\\n'+comment.body\n",
    "            df=df.append({'flair':flair,'title':submission.title,'author':submission.author,'text':submission.selftext,'url':submission.url,'comments':comments,'score':submission.score,'domain':submission.domain},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the database obtained into a csv file for further use during the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'reddit_flair.csv',index=False)\n",
    "df.to_csv(r'reddit_flair3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ran twice between a span of few days to get more recent data and saved the data from different days in different files, therefore one is commented out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II:\n",
    "\n",
    "Use Google BigQuery to get data from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import praw\n",
    "import os\n",
    "import datetime\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=r\"\"\n",
    "\n",
    "\n",
    "reddit=praw.Reddit(client_id='',\n",
    "                   client_secret='',\n",
    "                   username='',\n",
    "                   password='',\n",
    "                   user_agent=''\n",
    "                   )\n",
    "\n",
    "client=bigquery.Client()\n",
    "\n",
    "\n",
    "QUERY_POSTS=(\n",
    "'SELECT * '\n",
    "'FROM `fh-bigquery.reddit_posts.201*`'\n",
    "'WHERE subreddit = \"india\" and link_flair_text in (\"AskIndia\",\"Non-Political\",\"Scheduled\",\"Photography\",\"Science/Technology\",\"Politics\",\"Business/Finance\",\"Policy/Economy\",\"Sports\",\"Food\") ' \n",
    "'LIMIT 100000'\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY_POSTS)\n",
    "query = query_job.result().to_dataframe()\n",
    "\n",
    "\n",
    "keep = []\n",
    "data = query\n",
    "\n",
    "data.to_csv(r'reddit_flair2.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take specific features from the dataset only. And then to get a max. of 2000 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('reddit_flair2.csv')\n",
    "df3=df2[['link_flair_text','title','author','selftext','url','id','score','domain']]\n",
    "\n",
    "keep = []\n",
    "data = df3\n",
    "flairs = ['AskIndia','Non-Political','Scheduled','Photography','Science/Technology','Politics','Business/Finance','Policy/Economy','Sports','Food']\n",
    "for flair in flairs:\n",
    "    l = len(df3[df3['link_flair_text'] == flair])\n",
    "    if l > 2000:\n",
    "        l = 2000\n",
    "    idx = list(df3[df3['link_flair_text'] == flair]['id'])\n",
    "    lst = np.random.choice(idx, l, replace=False)\n",
    "    for item in lst:\n",
    "        keep.append(item)\n",
    "\n",
    "df4 = df3[df3['id'].isin(keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above snippet only got us the posts. We now need the comments too. For this we will use praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcomments(id_num):\n",
    "    submission=reddit.submission(id=id_num)\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    sub_comments=''\n",
    "    for i,comment in enumerate(submission.comments):\n",
    "        sub_comments+=comment.body\n",
    "        if i==10:\n",
    "            break\n",
    "    return sub_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['comments']=df4['id'].apply(getcomments)\n",
    "df4[['id','comments']].head()\n",
    "print('done')\n",
    "\n",
    "df4.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now collected and stored. The recent data is stored in reddit_flair and reddit_flair3 and the data from previous year is saved in out.csv .\n",
    "\n",
    "Now, we combine all the different data and save it in final_db_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1=pd.read_csv('reddit_flair3.csv')\n",
    "df1_2=pd.read_csv('reddit_flair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.concat([df1_1,df1_2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('out.csv')\n",
    "df2.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df2.drop('id',axis=1,inplace=True)\n",
    "df2.rename(columns={\"selftext\": \"text\",\"link_flair_text\":\"flair\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df1,df2],ignore_index=True)\n",
    "df_final.to_csv('final_db_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the EDA in the next notebook,we see that the dataset is very imbalanced. So oversampling is done here and it is stored in corrected_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('final_db_2.csv')\n",
    "df_final=pd.concat([\n",
    "    df,\n",
    "    df[df['flair']=='Scheduled'].sample(n=550),\n",
    "    df[df['flair']=='Food'].sample(n=600),\n",
    "    df[df['flair']=='Photography']\n",
    "])\n",
    "df_final=pd.concat([\n",
    "    df_final,\n",
    "    df_final[df_final['flair']=='Photography'].sample(n=500),\n",
    "])\n",
    "df_final.to_csv('corrected_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
